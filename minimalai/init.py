# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_minimalai-initialization.ipynb.

# %% auto 0
__all__ = ['clean_ipython_history', 'clean_traceback', 'clean_memory', 'BatchTransformCallback', 'GeneralRelu', 'plot_function',
           'initialize_conv_weights', 'lsuv_init', 'conv_layer', 'get_model']

# %% ../nbs/11_minimalai-initialization.ipynb 3
import pickle, gzip, math, os, time, shutil
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import sys,gc,traceback
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torch
import torchvision.transforms.functional as TF
import torch.nn.functional as F
from torch import tensor, nn, optim
from torch.utils.data import DataLoader, default_collate
from torch.nn import init
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset, load_dataset_builder

import fastcore.all as fc
from .datasets import *
from .conv import *
from .learner import *
from .activations import *

# %% ../nbs/11_minimalai-initialization.ipynb 14
def clean_ipython_history():
    # Code in this function mainly copied from IPython source
    if 'get_ipython' not in globals():
        return
    
    ipython_instance = get_ipython()
    user_namespace = ipython_instance.user_ns
    ipython_instance.displayhook.flush()
    prompt_count = ipython_instance.displayhook.prompt_count + 1
    for i in range(1, prompt_count):
        user_namespace.pop(f'_i{i}', None)
    user_namespace.update({'_i': '', '_ii': '', '_iii': ''})
    history_manager = ipython_instance.history_manager
    history_manager.input_hist_parsed[:] = [''] * prompt_count
    history_manager.input_hist_raw[:] = [''] * prompt_count
    history_manager._i = history_manager._ii = history_manager._iii = history_manager._i00 = ''

# %% ../nbs/11_minimalai-initialization.ipynb 15
def clean_traceback():
    # h/t Piotr Czapla
    if hasattr(sys, 'last_traceback'):
        traceback.clear_frames(sys.last_traceback)
        delattr(sys, 'last_traceback')
    if hasattr(sys, 'last_type'):
        delattr(sys, 'last_type')
    if hasattr(sys, 'last_value'):
        delattr(sys, 'last_value')

# %% ../nbs/11_minimalai-initialization.ipynb 16
def clean_memory():
    clean_traceback()
    clean_ipython_history()
    gc.collect()
    torch.cuda.empty_cache()

# %% ../nbs/11_minimalai-initialization.ipynb 84
class BatchTransformCallback(Callback):
    def __init__(self, transform, apply_on_train=True, apply_on_val=True):
        """
        Callback to apply a transformation `transform` to batches during training and/or validation.

        Args:
        - transform: The transformation function to apply to batches.
        - apply_on_train: Whether to apply the transformation during training batches.
        - apply_on_val: Whether to apply the transformation during validation batches.
        """
        self.transform = transform
        self.apply_on_train = apply_on_train
        self.apply_on_val = apply_on_val

    def before_batch(self, learn):
        """
        Apply the transformation to the batch before each training or validation batch if the corresponding flag is True.
        """
        if (self.apply_on_train and learn.training) or (self.apply_on_val and not learn.training):
            learn.batch = self.transform(learn.batch)

# %% ../nbs/11_minimalai-initialization.ipynb 92
class GeneralRelu(nn.Module):
    def __init__(self, negative_slope=None, subtract=None, max_value=None):
        """
        Custom implementation of a Rectified Linear Unit (ReLU) activation function with additional features.

        Args:
        - negative_slope (float, optional): The slope for the negative part of the function (leaky ReLU). If None, regular ReLU is used. Default: None.
        - subtract (float, optional): A value to subtract from the output tensor after applying the activation function. Default: None.
        - max_value (float, optional): The maximum value to clamp the output tensor. Values above this threshold will be set to max_value. Default: None.
        """
        super().__init__()
        self.negative_slope = negative_slope
        self.subtract = subtract
        self.max_value = max_value

    def forward(self, x): 
        """
        Forward pass through the Generalized ReLU activation function.

        Args:
        - x (torch.Tensor): Input tensor.

        Returns:
        - torch.Tensor: Output tensor after applying the activation function.
        """
        if self.negative_slope is not None:
            x = F.leaky_relu(x, negative_slope=self.negative_slope)
        else:
            x = F.relu(x)
        if self.subtract is not None:
            x -= self.subtract
        if self.max_value is not None:
            x = torch.clamp_max(x, max=self.max_value)
        return x

# %% ../nbs/11_minimalai-initialization.ipynb 93
def plot_function(function, start=-5., end=5., steps=100):
    """
    Plot the graph of a given function within a specified range.

    Args:
    - function (callable): The function to be plotted.
    - start (float, optional): The starting point of the range for the x-axis. Default: -5.0.
    - end (float, optional): The ending point of the range for the x-axis. Default: 5.0.
    - steps (int, optional): The number of steps to use for plotting. Default: 100.
    """
    x = torch.linspace(start, end, steps)
    plt.plot(x, function(x))
    plt.grid(True, which='both', ls='--')
    plt.axhline(y=0, color='k', linewidth=0.7)
    plt.axvline(x=0, color='k', linewidth=0.7)

# %% ../nbs/11_minimalai-initialization.ipynb 97
def initialize_conv_weights(module, leaky=0.):
    """
    Initialize the weights of convolutional layers using Kaiming normal initialization.
    
    Args:
    - module (nn.Module): The module for which to initialize weights.
    """
    if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
        init.kaiming_normal_(module.weight, a=leaky)

# %% ../nbs/11_minimalai-initialization.ipynb 106
def _lsuv_stats(hook, module, input, output):
    # Calculate the mean and standard deviation of the activations
    activations = to_cpu(output)
    hook.activation_mean = activations.mean()
    hook.activation_std = activations.std()

def lsuv_init(model, layer, layer_input, input_batch):
    # Initialize the LSUV algorithm
    activation_hook = Hook(layer, _lsuv_stats)
    with torch.no_grad():
        # Adjust the weights and biases until the mean and standard deviation of the activations are close to 0 and 1 respectively
        while model(input_batch) is not None and (abs(activation_hook.activation_std - 1) > 1e-3 or abs(activation_hook.activation_mean) > 1e-3):
            layer_input.bias -= activation_hook.activation_mean
            layer_input.weight.data /= activation_hook.activation_std
    # Remove the activation hook
    activation_hook.remove()

# %% ../nbs/11_minimalai-initialization.ipynb 117
def conv_layer(input_channels, output_channels, kernel_size=3, stride=2, activation=nn.ReLU, normalization=None, use_bias=None):
    """
    Create a convolutional layer with optional activation and normalization.

    Args:
    - input_channels (int): Number of input channels.
    - output_channels (int): Number of output channels.
    - kernel_size (int): Size of the convolutional kernel.
    - stride (int): Stride of the convolution operation.
    - activation (torch.nn.Module): Activation function to use.
    - normalization (torch.nn.Module): Normalization layer to apply.
    - use_bias (bool): Whether to use bias in the convolutional layer.

    Returns:
    - torch.nn.Module: Convolutional layer with optional activation and normalization.
    """
    if use_bias is None:
        use_bias = not isinstance(normalization, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))
    
    layers = [
        nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, bias=use_bias)
    ]
    if normalization:
        layers.append(normalization(output_channels))
    if activation:
        layers.append(activation())
    
    return nn.Sequential(*layers)

# %% ../nbs/11_minimalai-initialization.ipynb 118
def get_model(activation=nn.ReLU, num_filters=None, normalization=None):
    """
    Create a convolutional neural network model using the specified activation, number of filters, and normalization.

    Args:
    - activation (torch.nn.Module): Activation function to use.
    - num_filters (list): List of integers specifying the number of filters for each convolutional layer.
    - normalization (torch.nn.Module): Normalization layer to apply.

    Returns:
    - torch.nn.Module: Convolutional neural network model.
    """
    if num_filters is None:
        num_filters = [1, 8, 16, 32, 64]
    
    layers = [
        conv_layer(num_filters[i], num_filters[i+1], activation=activation, normalization=normalization)
        for i in range(len(num_filters)-1)
    ]
    return nn.Sequential(
        *layers,
        conv_layer(num_filters[-1], 10, activation=None, normalization=None, use_bias=True),
        nn.Flatten()
    ).to(default_device)
