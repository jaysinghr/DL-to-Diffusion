# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_minimalai-accelerate-sgd.ipynb.

# %% auto 0
__all__ = ['SGD', 'Momentum', 'RMSProp', 'Adam', 'plot_scheduler_learning_rates', 'BaseSchedulerCallback',
           'BatchSchedulerCallback', 'HasLearnerCallback', 'RecorderCallback', 'EpochSchedulerCallback']

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 3
import torch

from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 12
class SGD:
    """Stochastic Gradient Descent optimizer."""

    def __init__(self, parameters, learning_rate, weight_decay=0.):
        """
        Initializes the SGD optimizer.

        Args:
        - parameters (iterable): Iterable of parameters to optimize.
        - learning_rate (float): The learning rate.
        - weight_decay (float, optional): Weight decay (L2 penalty) (default: 0).
        """
        self.params = list(parameters)
        self.lr = learning_rate
        self.wd = weight_decay
        self.i = 0

    def step(self):
        """
        Performs a single optimization step.

        Updates the parameters based on the gradients and the learning rate.
        """
        with torch.no_grad():
            for param in self.params:
                self.regularization_step(param)
                self.optimization_step(param)
        self.i += 1

    def optimization_step(self, param):
        """
        Performs the optimization step for a single parameter.

        Updates the parameter based on its gradient and the learning rate.
        """
        param -= param.grad * self.lr

    def regularization_step(self, param):
        """
        Performs the regularization step for a single parameter.

        Applies weight decay (L2 penalty) if the weight decay is non-zero.
        """
        if self.wd != 0:
            param *= 1 - self.lr * self.wd

    def zero_grad(self):
        """Clears the gradients of all optimized parameters."""
        for param in self.params:
            if param.grad is not None:
                param.grad.detach_()
                param.grad.zero_()

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 19
class Momentum(SGD):
    """
    Momentum optimizer.

    Inherits from the SGD optimizer and adds momentum to the optimization step.
    """

    def __init__(self, parameters, learning_rate, weight_decay=0., momentum=0.9):
        """
        Initializes the Momentum optimizer.

        Args:
        - parameters (iterable): Iterable of parameters to optimize.
        - learning_rate (float): The learning rate.
        - weight_decay (float, optional): Weight decay (L2 penalty) (default: 0).
        - momentum (float, optional): Momentum factor (default: 0.9).
        """
        super().__init__(parameters, learning_rate=learning_rate, weight_decay=weight_decay)
        self.momentum = momentum

    def optimization_step(self, param):
        """
        Performs the optimization step with momentum for a single parameter.

        Updates the parameter based on its gradient, the learning rate, and momentum.
        """
        if not hasattr(param, 'grad_avg'):
            param.grad_avg = torch.zeros_like(param.grad)
        param.grad_avg = param.grad_avg * self.momentum + param.grad * (1 - self.momentum)
        param -= self.lr * param.grad_avg

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 24
class RMSProp(SGD):
    """
    RMSProp optimizer.

    Inherits from the SGD optimizer and implements the RMSProp optimization algorithm.
    """

    def __init__(self, parameters, learning_rate, weight_decay=0., squared_momentum=0.99, epsilon=1e-5):
        """
        Initializes the RMSProp optimizer.

        Args:
        - parameters (iterable): Iterable of parameters to optimize.
        - learning_rate (float): The learning rate.
        - weight_decay (float, optional): Weight decay (L2 penalty) (default: 0).
        - squared_momentum (float, optional): Squared momentum factor (default: 0.99).
        - epsilon (float, optional): Small constant to avoid division by zero (default: 1e-5).
        """
        super().__init__(parameters, learning_rate=learning_rate, weight_decay=weight_decay)
        self.squared_momentum = squared_momentum
        self.epsilon = epsilon

    def optimization_step(self, param):
        """
        Performs the RMSProp optimization step for a single parameter.

        Updates the parameter based on its gradient, the learning rate, squared momentum, and epsilon.
        """
        if not hasattr(param, 'squared_avg'):
            param.squared_avg = param.grad ** 2
        param.squared_avg = param.squared_avg * self.squared_momentum + param.grad ** 2 * (1 - self.squared_momentum)
        param -= self.lr * param.grad / (param.squared_avg.sqrt() + self.epsilon)

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 28
class Adam(SGD):
    """
    Adam optimizer.

    Inherits from the SGD optimizer and implements the Adam optimization algorithm.
    """

    def __init__(self, parameters, learning_rate, weight_decay=0., beta1=0.9, beta2=0.99, epsilon=1e-5):
        """
        Initializes the Adam optimizer.

        Args:
        - parameters (iterable): Iterable of parameters to optimize.
        - learning_rate (float): The learning rate.
        - weight_decay (float, optional): Weight decay (L2 penalty) (default: 0).
        - beta1 (float, optional): Exponential decay rate for the first moment estimates (default: 0.9).
        - beta2 (float, optional): Exponential decay rate for the second moment estimates (default: 0.99).
        - epsilon (float, optional): Small constant to avoid division by zero (default: 1e-5).
        """
        super().__init__(parameters, learning_rate=learning_rate, weight_decay=weight_decay)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

    def optimization_step(self, param):
        """
        Performs the Adam optimization step for a single parameter.

        Updates the parameter based on its gradient, the learning rate, beta1, beta2, and epsilon.
        """
        if not hasattr(param, 'avg'):
            param.avg = torch.zeros_like(param.grad.data)
        if not hasattr(param, 'squared_avg'):
            param.squared_avg = torch.zeros_like(param.grad.data)

        param.avg = self.beta1 * param.avg + (1 - self.beta1) * param.grad
        unbias_avg = param.avg / (1 - (self.beta1 ** (self.i + 1)))

        param.squared_avg = self.beta2 * param.squared_avg + (1 - self.beta2) * (param.grad ** 2)
        unbias_sqr_avg = param.squared_avg / (1 - (self.beta2 ** (self.i + 1)))

        param -= self.lr * unbias_avg / (unbias_sqr_avg + self.epsilon).sqrt()

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 43
def plot_scheduler_learning_rates(scheduler, steps):
    """
    Plot learning rates over a series of optimization steps using a scheduler.

    Args:
    - scheduler: The learning rate scheduler.
    - steps (int): The number of optimization steps.
    """
    learning_rates = [scheduler.get_last_lr()]
    for _ in range(steps):
        scheduler.optimizer.step()
        scheduler.step()
        learning_rates.append(scheduler.get_last_lr())
    plt.plot(learning_rates)
    plt.xlabel('Steps')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule')
    plt.show()

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 46
class BaseSchedulerCallback(Callback):
    """
    Base callback for learning rate scheduling.

    This callback is designed to work with a learning rate scheduler and is intended to be used during training.
    """

    def __init__(self, scheduler):
        """
        Initializes the BaseSchedCallback with a learning rate scheduler.

        Args:
        - scheduler: The learning rate scheduler.
        """
        self.scheduler = scheduler

    def before_fit(self, learner):
        """
        Callback method called before the training loop starts.

        This method initializes the scheduler for the optimizer used in the learner.
        """
        self.scheduler_obj = self.scheduler(learner.optimizer)

    def _step(self, learner):
        """
        Internal method to perform a scheduler step.

        This method is called during the training loop to update the scheduler.
        """
        if learner.training:
            self.scheduler_obj.step()

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 47
class BatchSchedulerCallback(BaseSchedulerCallback):
    """
    Callback for a scheduler that updates after each batch.

    Inherits from BaseSchedulerCallback and performs a step of the scheduler after each batch during training.
    """

    def after_batch(self, learner):
        """
        Callback after processing each batch.

        Performs a step of the scheduler if the learner is in training mode.
        """
        self._step(learner)

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 48
class HasLearnerCallback(Callback):
    """
    Callback for storing a reference to the learner during training.

    Inherits from Callback and stores a reference to the learner during training.
    """

    def before_fit(self, learner):
        """
        Callback before fitting the model.

        Stores a reference to the learner.
        """
        self.learner = learner

    def after_fit(self, learner):
        """
        Callback after fitting the model.

        Resets the reference to the learner.
        """
        self.learner = None

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 49
class RecorderCallback(Callback):
    """
    Callback for recording and plotting metrics during training.

    Inherits from Callback and records metrics specified in the dictionary `d` during training.
    """

    def __init__(self, **kwargs):
        """
        Initializes the RecorderCallback.

        Args:
        - kwargs: Dictionary of metrics to record.
        """
        self.metrics = kwargs

    def before_fit(self, learner):
        """
        Callback before fitting the model.

        Initializes the records dictionary and gets the parameter group from the optimizer.
        """
        self.records = {k: [] for k in self.metrics.keys()}
        self.param_group = learner.optimizer.param_groups[0]

    def after_batch(self, learner):
        """
        Callback after processing each batch.

        Records the metrics specified in the dictionary `d` for each batch.
        """
        if not learner.training:
            return
        for key, value_func in self.metrics.items():
            self.records[key].append(value_func(self))

    def plot(self):
        """
        Plots the recorded metrics.
        """
        for key, values in self.records.items():
            plt.plot(values, label=key)
            plt.legend()
            plt.show()

# %% ../nbs/12_minimalai-accelerate-sgd.ipynb 55
class EpochSchedulerCallback(BaseSchedulerCallback):
    """
    Callback for a scheduler that updates after each epoch.

    Inherits from BaseSchedulerCallback and performs a step of the scheduler after each epoch during training.
    """

    def after_epoch(self, learner):
        """
        Callback after completing each epoch.

        Performs a step of the scheduler if the learner is in training mode.
        """
        self._step(learner)
